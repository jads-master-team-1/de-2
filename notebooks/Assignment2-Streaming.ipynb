{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe06ed6-7385-4ef7-baa2-e00c28f33b6d",
   "metadata": {},
   "source": [
    "## Streaming Pipeline- Medals Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae8b22-356c-48b1-9c4d-3d8130bcd9e0",
   "metadata": {},
   "source": [
    "Here we imagine that the games are taking place, the results are being streamed in so they can be displayed on our dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8720a44-948d-4a11-a00b-1049b58f533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting up Spark configuration and necessary existing tables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit, from_csv\n",
    "\n",
    "# Setting Spark configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"SparkStreamAssignment2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "#Have to rename bucket\n",
    "bucket = \"dejads_temp_assignment2_team1\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Importing country tables from bucket so they can be used in a join with our stream data\n",
    "country_file_path = 'gs://dejads_output_assignment2_team1/country.csv/*.csv' \n",
    "\n",
    "country_df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(country_file_path)\n",
    "country_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd2c34d-e73c-4497-97eb-b36e403d8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_csv(value): struct (nullable = true)\n",
      " |    |-- medal_type: string (nullable = true)\n",
      " |    |-- medal_code: integer (nullable = true)\n",
      " |    |-- medal_date: string (nullable = true)\n",
      " |    |-- athlete_short_name: string (nullable = true)\n",
      " |    |-- athlete_name: string (nullable = true)\n",
      " |    |-- athlete_sex: string (nullable = true)\n",
      " |    |-- athlete_link: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- discipline_code: string (nullable = true)\n",
      " |    |-- event: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- medal_type: string (nullable = true)\n",
      " |-- medal_code: integer (nullable = true)\n",
      " |-- medal_date: string (nullable = true)\n",
      " |-- athlete_short_name: string (nullable = true)\n",
      " |-- athlete_name: string (nullable = true)\n",
      " |-- athlete_sex: string (nullable = true)\n",
      " |-- athlete_link: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- discipline_code: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting up streaming settings\n",
    "from pyspark.sql.functions import count, max, current_timestamp, col, desc\n",
    "\n",
    "# Setting schema of data prior to reading stream\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"medal_type\", StringType(), True),\n",
    "     StructField(\"medal_code\", IntegerType(), True),\n",
    "     StructField(\"medal_date\", StringType(), True),\n",
    "     StructField(\"athlete_short_name\", StringType(), True),\n",
    "     StructField(\"athlete_name\", StringType(), True),\n",
    "     StructField(\"athlete_sex\", StringType(), True),\n",
    "     StructField(\"athlete_link\", StringType(), True),\n",
    "     StructField(\"country_code\", StringType(), True),\n",
    "     StructField(\"discipline_code\", StringType(), True),\n",
    "     StructField(\"event\", StringType(), True)\n",
    "     ])\n",
    "\n",
    "# Reading from Kafka stream of medal results\n",
    "# Don't want stream to fail in case of potential data loss\n",
    "# Subscribing to medal topic\n",
    "# Want to pick up all medal data available on topic so using startingOffsets = earliest\n",
    "kafkaStream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "  .option(\"failOnDataLoss\", \"false\") \\\n",
    "  .option(\"subscribe\", \"medals\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "# Cast Kafka received values to the correct column structure\n",
    "df = kafkaStream.selectExpr(\"CAST(value AS STRING)\")\n",
    "df1 = df.select(from_csv(df.value, dataSchema.simpleString()))\n",
    "df1.printSchema()\n",
    "sdf = df1.select(col(\"from_csv(value).*\"))\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f376f22-f382-41b7-90c9-bbbdb4be6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for producing medals table\n",
    "from pyspark.sql.functions import countDistinct, when, col, count, desc\n",
    "\n",
    "# Selecting all distinct rows for country, medal type, discipline and event (because medal won by a team will appear multiple times in dataset)\n",
    "df = sdf.select('medal_type',  \"medal_code\", \"event\", \"country_code\", \"discipline_code\", \"event\").distinct()\n",
    "# Joining to country table to get country column and dropping country code\n",
    "df = df.join(country_df, \"country_code\", \"left_outer\").drop('country_code')\n",
    "# Counting the number of gold, silver and bronze medals\n",
    "df = df.groupBy(\"country\").agg(\n",
    "    count(when(col(\"medal_code\") == 1, 1)),\n",
    "    count(when(col(\"medal_code\") == 2, 1)),\n",
    "    count(when(col(\"medal_code\") == 3, 1)),\n",
    ").withColumnRenamed(\n",
    "    'count(CASE WHEN (medal_code = 1) THEN 1 END)', 'gold_medal_count'\n",
    ").withColumnRenamed(\n",
    "    'count(CASE WHEN (medal_code = 2) THEN 1 END)', 'silver_medal_count'\n",
    ").withColumnRenamed(\n",
    "    'count(CASE WHEN (medal_code = 3) THEN 1 END)', 'bronze_medal_count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9792775-5318-4f07-af96-b1245db7bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Writing the data to BigQuery in batches\n",
    "def foreach_batch_function_totalmedals(df, batch_id):\n",
    "    # Saving the data to BigQuery as batch processing sink - see, use write(), save(), etc.\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'de2021-assignment2.assignment2.medals_table') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "query = df.writeStream.outputMode(\"complete\") \\\n",
    "                   .trigger(processingTime = '2 seconds').foreachBatch(foreach_batch_function_totalmedals).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c8cc0d-ea9d-4ef3-b34a-41ef7ac21276",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
